# Synapse-HandsOn-QuickStart
Synapse Get Started Guide

Please follow the links in sequence to explore the features and functionalities of Synapse. 


Overview on technical capabilities : https://github.com/chbasani/Synapse-HandsOn-QuickStart/wiki

Creation of a workspace : https://github.com/chbasani/Synapse-HandsOn-QuickStart/wiki/Workspace

Creation of a Data lake and using it for Synapse workspace : https://github.com/chbasani/Synapse-HandsOn-QuickStart/wiki/Data-Lake-Permissions-,-Spark-pool

Data exploration in Synpase workspace : https://github.com/chbasani/Synapse-HandsOn-QuickStart/wiki/Data-exploration

Spark processing for file formats  , Regression analysis , AutoML experiment: https://github.com/chbasani/Synapse-HandsOn-QuickStart/wiki/Spark-process-,-modeling-,-AutoML

Data Factory capabilities of Copy , Data Flow and publish/exeucte a pipeline : https://github.com/chbasani/Synapse-HandsOn-QuickStart/wiki/Data-Factory---Copy-,-Data-Flow-,-ETL-process

PowerBI workspace and create a dashboard : https://github.com/chbasani/Synapse-HandsOn-QuickStart/wiki/Power-BI

( The same content as the above weblinks is in the docx format . You can click the document to start with, download the raw document. Follow step by step directions in the document.)

Synapse Analytics - Capabilities for Analytic / Exploratory environment

1.	Data 
a.	Configure linked services (Preview data , schema  - Json format , csv etc) 
b.	Realtime Streaming data – Synapse Link for Cosmos DB
c.	Custom Data Connectors /activity

2.	File formats supported – structured, unstructured

3.	Explore data in data source 
a.	Select top
b.	SQL on demand 
c.	Open rowset
d.	Bulk load
e.	Load data with spark notebook

4.	Coding experience 
a.	Import / Export / Clone
b.	Integration with Visual Studio, SSMS, Synapse workspace
c.	Debug 
d.	Process execution status 
e.	Schedule in the Runtime environment/ Trigger 
f.	Future roadmap – 
i.	CI/CD - DevOps / Git enabled in future (already available in Data Factory)
ii.	Python Kernel 
iii.	Auto ML: No-code experience

5.	Data Manipulation 
a.	SQL 
b.	Spark code
c.	ETL with Data Factory (No-code, Low-code)
d.	Virtual Datawarehouse

6.	SQL capabilities  
a.	Ansi SQL
b.	On- Demand SQL 
c.	CETAS
d.	Materialized views
e.	Result-set caching

7.	Spark capabilities
a.	Create a spark notebook for the SQL table read   
b.	Python libraries for modeling 
c.	Auto ML
d.	ONNX – to move the model across different platforms 
e.	Time travel for delta lake using spark

8.	Data Factory
a.	Reading a data source
b.	Orchestration- pipelines and execution

9.	Power BI 
a.	Connect to a PBI workspace and develop insights/visualizations

10.	Security
a.	Managed VNets
b.	IP firewall rules
c.	Endpoint
d.	Threat detection
e.	Row Level security 
f.	Column level security
g.	Dynamic data masking
h.	Transparent Data Encryption
i.	Synapse Roles – Workspace admin, sql admin, spark admin
*Active directory / single sign on 
*Underlying access permissions for the data lake 

11.	SQL Server management features – Transparent Data Encryption, Activity logs (identify the sql statements executed), Monitoring Query activity, Geobackup, Maintenance Schedule 

12.	Spark management features – Monitoring, Activity logs, metrics; Package config file 

13.	SQL Distribution 
a.	HASH distributed
b.	Round Robin
c.	Replicated
d.	Index 
i.	Clustered Columnstore Index 
ii.	Clustered Index 
iii.	Heap 
iv.	Nonclustered indexes
v.	Ordered Clustered Columnstore Index

